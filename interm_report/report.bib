@article{Demsar2016BalancedMixture,
    title = {{A Balanced Mixture of Antagonistic Pressures Promotes the Evolution of Parallel Movement}},
    year = {2016},
    journal = {Scientific Reports},
    author = {Dem{\v{s}}ar, Jure and {\v{S}}trumbelj, Erik and Lebar Bajec, Iztok},
    volume = {6},
    doi = {10.1038/srep39428}
}

@article{Demsar2017LinguisticEvolution,
    title = {{Evolution of Collective Behaviour in an Artificial World Using Linguistic Fuzzy Rule-Based Systems}},
    year = {2017},
    journal = {PLoS ONE},
    author = {Dem{\v{s}}ar, Jure and Lebar Bajec, Iztok},
    number = {1},
    pages = {1--20},
    volume = {12},
    doi = {10.1371/journal.pone.0168876}
}

@inproceedings{warren2022better,
  title={“better” counterfactuals, ones people can understand: psychologically-plausible case-based counterfactuals using categorical features for explainable AI (XAI)},
  author={Warren, Greta and Smyth, Barry and Keane, Mark T},
  booktitle={International conference on case-based reasoning},
  pages={63--78},
  year={2022},
  organization={Springer}
}

@inproceedings{gomez2020vice,
  title={Vice: Visual counterfactual explanations for machine learning models},
  author={Gomez, Oscar and Holter, Steffen and Yuan, Jun and Bertini, Enrico},
  booktitle={Proceedings of the 25th international conference on intelligent user interfaces},
  pages={531--535},
  year={2020}
}

@article{garcia2022sda,
  title={SDA-Vis: A visualization system for student dropout analysis based on counterfactual exploration},
  author={Garcia-Zanabria, Germain and Gutierrez-Pachas, Daniel A and Camara-Chavez, Guillermo and Poco, Jorge and Gomez-Nieto, Erick},
  journal={Applied Sciences},
  volume={12},
  number={12},
  pages={5785},
  year={2022},
  publisher={MDPI}
}
@inproceedings{guyomard2023interactive,
  title={Interactive Visualization of Counterfactual Explanations for Tabular Data},
  author={Guyomard, Victor and Fessant, Fran{\c{c}}oise and Guyet, Thomas and Bouadi, Tassadit and Termier, Alexandre},
  booktitle={Joint European Conference on Machine Learning and Knowledge Discovery in Databases},
  pages={330--334},
  year={2023},
  organization={Springer}
}

@inproceedings{gomez2021advice,
  title={Advice: Aggregated visual counterfactual explanations for machine learning model validation},
  author={Gomez, Oscar and Holter, Steffen and Yuan, Jun and Bertini, Enrico},
  booktitle={2021 IEEE Visualization Conference (VIS)},
  pages={31--35},
  year={2021},
  organization={IEEE}
}

@article{cheng2020dece,
  title={Dece: Decision explorer with counterfactual explanations for machine learning models},
  author={Cheng, Furui and Ming, Yao and Qu, Huamin},
  journal={IEEE Transactions on Visualization and Computer Graphics},
  volume={27},
  number={2},
  pages={1438--1447},
  year={2020},
  publisher={IEEE}
}

@inproceedings{warren_et_al_user_study_2023,
author = {Warren, Greta and Byrne, Ruth M. J. and Keane, Mark T.},
title = {Categorical and Continuous Features in Counterfactual Explanations of AI Systems},
year = {2023},
isbn = {9798400701061},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3581641.3584090},
doi = {10.1145/3581641.3584090},
abstract = {Recently, eXplainable AI (XAI) research has focused on the use of counterfactual explanations to address interpretability, algorithmic recourse, and bias in AI system decision-making. The proponents of these algorithms claim they meet users’ requirements for counterfactual explanations. For instance, many claim that the output of their algorithms work as explanations because they prioritise "plausible", "actionable" or "causally important" features in their generated counterfactuals. However, very few of these claims have been tested in controlled psychological studies, and we know very little about which aspects of counterfactual explanations help users to understand AI system decisions. Furthermore, we do not know whether counterfactual explanations are an advance on more traditional causal explanations that have a much longer history in AI (in explaining expert systems and decision trees). Accordingly, we carried out two user studies to (i) test a fundamental distinction in feature-types, between categorical and continuous features, and (ii) compare the relative effectiveness of counterfactual and causal explanations. The studies used a simulated, automated decision-making app that determined safe driving limits after drinking alcohol, based on predicted blood alcohol content, and user responses were measured objectively (users’ predictive accuracy) and subjectively (users’ satisfaction and trust judgments). Study 1 (N=127) showed that users understand explanations referring to categorical features more readily than those referring to continuous features. It also discovered a dissociation between objective and subjective measures: counterfactual explanations elicited higher accuracy of predictions than no-explanation control descriptions but no higher accuracy than causal explanations, yet counterfactual explanations elicited greater satisfaction and trust judgments than causal explanations. Study 2 (N=211) found that users were more accurate for categorically-transformed features compared to continuous ones, and also replicated the results of Study 1. The findings delineate important boundary conditions for current and future counterfactual explanation methods in XAI.},
booktitle = {Proceedings of the 28th International Conference on Intelligent User Interfaces},
pages = {171–187},
numpages = {17},
keywords = {XAI, counterfactual, explanation, user study},
location = {<conf-loc>, <city>Sydney</city>, <state>NSW</state>, <country>Australia</country>, </conf-loc>},
series = {IUI '23}
}

@misc{keane2020good,
      title={Good Counterfactuals and Where to Find Them: A Case-Based Technique for Generating Counterfactuals for Explainable AI (XAI)}, 
      author={Mark T. Keane and Barry Smyth},
      year={2020},
      eprint={2005.13997},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}